CPE 419 Project #1 Part C Report

What is the CGMA (compute to global memory access) ratio for your code? This is the number of floating-point calculations performed for each access to the global memory within a region of a CUDA program. How does this compare to the CGMA ratio for your solution to Part B?

The CGMA ratio for Part B was 1, there were 128 global memory reads required (64 for each array) for 128 computations (64 multiplications and 64 additions).

In Part C, each thread has 2 global memory accesses and 64 additions and 64 multiplications for a total of 128/2 = 64 CGMA ratio

Therefore in Part C our CGMA is 64 times better.

PART C:
Sample Runs, single precision:

  Run # |   Real   |   User   |    Sys   | 
------------------------------------------
    1   |  2.623s  |  2.053s  |  0.370s  |
------------------------------------------
    2   |  2.565s  |  2.036s  |  0.377s  |
------------------------------------------
    3   |  2.586s  |  2.048s  |  0.367s  |
    
Sample Runs, double precision:

  Run # |   Real   |   User   |    Sys   | 
------------------------------------------
    1   |  2.636s  |  2.067s  |  0.405s  |
------------------------------------------
    2   |  2.627s  |  2.077s  |  0.402s  |
------------------------------------------
    3   |  2.624s  |  2.080s  |  0.386s  |
  
  

CPE 419 Project #1 Part D Report


Sample Runs, single precision:

  Run # |   Real   |   User   |    Sys   | 
------------------------------------------
    1   |  2.540s  |  2.051s  |  0.267s  |
------------------------------------------
    2   |  2.520s  |  2.032s  |  0.284s  |
------------------------------------------
    3   |  2.536s  |  2.037s  |  0.274s  |
    
    
Double precision not allowed with cuBLAS library
